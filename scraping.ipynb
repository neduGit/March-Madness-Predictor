{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Trend Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Save the URL of the link that has all D1 college basketball schools to ever play\n",
    "schools_url = \"https://www.sports-reference.com/cbb/schools/\"\n",
    "\n",
    "#Extract the page using requests and beautiful soup from the URL\n",
    "response = requests.get(schools_url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "#Find and save the schools table on the page\n",
    "table = soup.find(\"table\", id=\"NCAAM_schools\")\n",
    "\n",
    "#Iterate through each row in the table\n",
    "#tbody denotes the rows instead of rows and column names\n",
    "teams = []\n",
    "for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "\n",
    "    #Find each cell by locating where td is in the name\n",
    "    #This provides the values for each column for a given row\n",
    "    td_cells = row.find_all(\"td\")\n",
    "\n",
    "    if td_cells:\n",
    "\n",
    "        #Extract the first column for each row\n",
    "        #This column contains the team name, which is a link that takes you to the team's page\n",
    "        td = td_cells[0]  # Get the first <td> (team name)\n",
    "\n",
    "        #Extract the link from this clickable cell\n",
    "        link = td.find(\"a\")  # Find the <a> tag inside the <td>\n",
    "\n",
    "        #Extract the portion of the link that is the team id and save it \n",
    "        if link:\n",
    "            team_url = link[\"href\"]\n",
    "            team_name = team_url.split(\"/schools/\")[-1].split(\"/\")[0]\n",
    "            \n",
    "            #Only add the team to the teams list if the to_year column is 2002 or later, meaning the team has been D1 since 2002\n",
    "            try:\n",
    "                to_year = td_cells[3].text.strip()  # Get the \"To\" year from the 4th <td> (index 3)\n",
    "                if int(to_year) >= 2002:\n",
    "                    teams.append(team_name)\n",
    "\n",
    "            #Skip the team if it has not been D1 since 2002\n",
    "            except ValueError:\n",
    "                pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first code chunk scrapes all of the team IDs for teams that have been D1 since 2002. These IDs can be used to iterate through since they are the aspects of each URL that differentiates it from other teams. Within a URL on Sports Reference, for team stats, there is a team id and a season that determine which team's stats you want to see and for what year. This above code allows me to scrape all of these team IDs so that I can iterate through them for the following loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "#Create the function to scrape game data taking in a url and a team id\n",
    "def scrape_game_data(url, team):\n",
    "\n",
    "    #Store the page from the given url\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    #Extract commented out HTML that might contain hidden tables\n",
    "    comments = soup.find_all(string = lambda text: isinstance(text, Comment))\n",
    "\n",
    "    #Iterate through these commented out HTMLs and if it is a table, add it to the soup\n",
    "    #This allows the function to scrape the tables even if they are not stored as a table\n",
    "    for comment in comments:\n",
    "        if \"<table\" in comment:\n",
    "            soup.append(BeautifulSoup(comment, \"html.parser\"))\n",
    "    \n",
    "    #Create the table IDs using the team that you are looking for\n",
    "    table_ids = [\n",
    "        f\"box-score-basic-{team}\",\n",
    "        f\"box-score-advanced-{team}\"\n",
    "    ]\n",
    "\n",
    "    #Create an empty object to store the data frmes\n",
    "    dfs = {}\n",
    "\n",
    "    #Iterate through the two table ids\n",
    "    for table_id in table_ids:\n",
    "\n",
    "        #Find the given table\n",
    "        table = soup.find(\"table\", {\"id\": table_id})\n",
    "\n",
    "        if table:\n",
    "\n",
    "            #Retrieve the column names by finding the \"head\" column\n",
    "            headers = [th.text.strip() for th in table.find(\"thead\").find_all(\"th\")]\n",
    "\n",
    "            #Only take the second row of headers because the first is a table name\n",
    "            headers = headers[2:len(headers)]\n",
    "            \n",
    "            #Extract player rows\n",
    "            players = [row.find(\"th\").text.strip() for row in table.find(\"tfoot\").find_all(\"tr\")]\n",
    "            \n",
    "            # Extract the bottom row that gives team totals\n",
    "            rows = []\n",
    "            for row in table.find(\"tfoot\").find_all(\"tr\"):\n",
    "\n",
    "                #Take each cell that gives the team totals and append them\n",
    "                cells = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "                if cells:\n",
    "                    rows.append(cells)\n",
    "            \n",
    "            #Convert to dataframe with the column names as the headers extracted from above\n",
    "            df = pd.DataFrame(rows, columns = headers[1:])\n",
    "\n",
    "            #Insert player names as the first column\n",
    "            df.insert(0, \"Player\", players)  \n",
    "            dfs[table_id] = df\n",
    "\n",
    "    #Merge the two dataframes on the player column, which would be \"School Totals\" as that is the only row left\n",
    "    if f\"box-score-basic-{team}\" in dfs and f\"box-score-advanced-{team}\" in dfs:\n",
    "        merged_df = pd.merge(dfs[f\"box-score-basic-{team}\"], dfs[f\"box-score-advanced-{team}\"], on=\"Player\", how=\"inner\")\n",
    "\n",
    "        #Return the merged dataframe\n",
    "        return merged_df\n",
    "\n",
    "    else:\n",
    "        return \"Does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code chunk creates a function that will be used in the following for loop as well. The function takes in a url, specifically a url for a game, and scrapes the simple box score table and the advanced box score table for the given team. It takes the bottom row of both of these tables, which is the row for team totals, and if the function is able to find both of these tables, then it merges them together. This outputs a single row of data that has the team totals for both basic and advanced metrics for the given game. If the function is not able to find both tables, then it returns \"Does not Exist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 out of 8880: Processing 2002 duke...\n",
      "Error before printing: list index out of range\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2003 duke...\n",
      "Error before printing: list index out of range\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2004 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 72, in <module>\n",
      "    selected_urls = [urls[i] for i in dates]\n",
      "                     ~~~~^^^\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 72, in <module>\n",
      "    selected_urls = [urls[i] for i in dates]\n",
      "                     ~~~~^^^\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before printing: list index out of range\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2005 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 72, in <module>\n",
      "    selected_urls = [urls[i] for i in dates]\n",
      "                     ~~~~^^^\n",
      "IndexError: list index out of range\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 125, in <module>\n",
      "    conf_tournament_selected_urls = [conf_tournament_urls[i] for i in conf_tournament_dates]\n",
      "                                     ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before printing: list index out of range\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2006 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 0\n",
      "Rows after appending: 3\n",
      "1 Teams have been added\n",
      "Iteration 1: Sleeping for 4.73 seconds...\n",
      "Iteration 1 out of 8880: Processing 2007 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 3\n",
      "Rows after appending: 4\n",
      "2 Teams have been added\n",
      "Iteration 1: Sleeping for 4.76 seconds...\n",
      "Iteration 1 out of 8880: Processing 2008 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 4\n",
      "Rows after appending: 6\n",
      "3 Teams have been added\n",
      "Iteration 1: Sleeping for 3.93 seconds...\n",
      "Iteration 1 out of 8880: Processing 2009 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 6\n",
      "Rows after appending: 9\n",
      "4 Teams have been added\n",
      "Iteration 1: Sleeping for 4.67 seconds...\n",
      "Iteration 1 out of 8880: Processing 2010 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 9\n",
      "Rows after appending: 15\n",
      "5 Teams have been added\n",
      "Iteration 1: Sleeping for 2.10 seconds...\n",
      "Iteration 1 out of 8880: Processing 2011 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 15\n",
      "Error before printing: Reindexing only valid with uniquely valued Index objects\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2012 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 177, in <module>\n",
      "    all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 395, in concat\n",
      "    return op.get_result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 680, in get_result\n",
      "    indexers[ax] = obj_labels.get_indexer(new_labels)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3885, in get_indexer\n",
      "    raise InvalidIndexError(self._requires_unique_msg)\n",
      "pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 15\n",
      "Error before printing: Reindexing only valid with uniquely valued Index objects\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2013 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 177, in <module>\n",
      "    all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 395, in concat\n",
      "    return op.get_result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 680, in get_result\n",
      "    indexers[ax] = obj_labels.get_indexer(new_labels)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3885, in get_indexer\n",
      "    raise InvalidIndexError(self._requires_unique_msg)\n",
      "pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 15\n",
      "Error before printing: Reindexing only valid with uniquely valued Index objects\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2014 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 177, in <module>\n",
      "    all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 395, in concat\n",
      "    return op.get_result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 680, in get_result\n",
      "    indexers[ax] = obj_labels.get_indexer(new_labels)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3885, in get_indexer\n",
      "    raise InvalidIndexError(self._requires_unique_msg)\n",
      "pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 15\n",
      "Error before printing: Reindexing only valid with uniquely valued Index objects\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2015 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 177, in <module>\n",
      "    all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 395, in concat\n",
      "    return op.get_result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 680, in get_result\n",
      "    indexers[ax] = obj_labels.get_indexer(new_labels)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3885, in get_indexer\n",
      "    raise InvalidIndexError(self._requires_unique_msg)\n",
      "pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 15\n",
      "Error before printing: Reindexing only valid with uniquely valued Index objects\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2016 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 177, in <module>\n",
      "    all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 395, in concat\n",
      "    return op.get_result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 680, in get_result\n",
      "    indexers[ax] = obj_labels.get_indexer(new_labels)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3885, in get_indexer\n",
      "    raise InvalidIndexError(self._requires_unique_msg)\n",
      "pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before appending: 15\n",
      "Error before printing: Reindexing only valid with uniquely valued Index objects\n",
      "Skipping to the next team...\n",
      "\n",
      "Iteration 1 out of 8880: Processing 2017 duke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1562/228980485.py\", line 177, in <module>\n",
      "    all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 395, in concat\n",
      "    return op.get_result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/reshape/concat.py\", line 680, in get_result\n",
      "    indexers[ax] = obj_labels.get_indexer(new_labels)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py\", line 3885, in get_indexer\n",
      "    raise InvalidIndexError(self._requires_unique_msg)\n",
      "pandas.errors.InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
      "/tmp/ipykernel_1562/228980485.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m conf_tournament_selected_urls:\n\u001b[32m    131\u001b[39m     one_conf_tournament_game = scrape_game_data(link, team)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(one_conf_tournament_game, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConference Tournament Game Table not Found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "#Create a variable that tracks how many teams data is collected for\n",
    "teams_added = 0\n",
    "\n",
    "#Create all the empty dataframes that will be populated throughout the loop\n",
    "all_tournament_games = pd.DataFrame()\n",
    "all_conf_tournament_games = pd.DataFrame()\n",
    "all_regular_season_end = pd.DataFrame()\n",
    "\n",
    "#Create the seasons that are going to be scraped\n",
    "seasons = list(range(2002, 2026))[::-1]\n",
    "for season in seasons:\n",
    "\n",
    "    #Begin the loop for teams. Use a try catch block so that an error won't stop the loop when we are far along\n",
    "    for idx, team in enumerate(teams, start=1):\n",
    "        try:\n",
    "            #Print the iteration that the loop is on each time\n",
    "            print(f\"Iteration {idx} out of 370: Processing {season} {team}...\")\n",
    "\n",
    "            #Store the URL with the given team and season and get the page\n",
    "            url = f\"https://www.sports-reference.com/cbb/schools/{team}/men/{season}-schedule.html\"\n",
    "            response = requests.get(url)\n",
    "\n",
    "            #If there is an issue with the URL, skip it and print that it was not found\n",
    "            if response.status_code != 200 and response.status_code != 429:\n",
    "                print(f\"Skipping {team}, URL not found.\")\n",
    "                continue\n",
    "            \n",
    "            #If the site locks you out, print this message and stop the loop\n",
    "            if response.status_code == 429:\n",
    "                print(\"Too Many Requests. Stopping loop. Try again in 30 minutes\")\n",
    "                break\n",
    "            \n",
    "            #Store the information on the page\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            #Store the schedule table from the page\n",
    "            table = soup.find(\"table\", id=\"schedule\")\n",
    "\n",
    "            #If no table is found, print that it wasn't found and skip the iteration\n",
    "            if table is None:\n",
    "                print(f\"No schedule table found for {team} (Iteration {idx}). Skipping...\")\n",
    "                continue\n",
    "\n",
    "            #Extract the column names\n",
    "            headers = [th.text for th in table.find(\"thead\").find_all(\"th\")]\n",
    "\n",
    "            #Extract the rows similar to previous scraping\n",
    "            rows = []\n",
    "            for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "                cols = [td.text.strip() for td in tr.find_all([\"th\", \"td\"])]\n",
    "                if cols:  # Ignore empty rows\n",
    "                    rows.append(cols)\n",
    "\n",
    "            #Create a dataframe with all the team's scheduled games from the rows and column names\n",
    "            all_games = pd.DataFrame(rows, columns = headers)\n",
    "\n",
    "            #Add a column for team season and the conference the team is in\n",
    "            all_games[\"team\"] = team\n",
    "            all_games[\"season\"] = season\n",
    "            all_games[\"conference\"] = all_games[\"Conf\"].mode()[0]\n",
    "\n",
    "            #Check if the team had an NCAA tournament game, if they did not, then skip them\n",
    "            if (len(all_games[all_games[\"Type\"] == \"NCAA\"]) > 0):\n",
    "                \n",
    "                #Extract all the NCAA tournament games\n",
    "                tournament_games = all_games[all_games[\"Type\"] == \"NCAA\"]\n",
    "\n",
    "                #Extract the last five games of the team's regular season\n",
    "                regular_season_end = all_games[all_games[\"Type\"] == \"REG\"].tail(5)\n",
    "                \n",
    "                #Change the game number to numeric and store it as the table's index by subtracting one\n",
    "                regular_season_end[\"G\"] = pd.to_numeric(regular_season_end[\"G\"])\n",
    "                dates = (regular_season_end[\"G\"] - 1).tolist()\n",
    "\n",
    "                #Find the clickable links from the date column\n",
    "                date_links = soup.select(\"td[data-stat='date_game'] a\")\n",
    "\n",
    "                #Extract the five links from the last five regular season games\n",
    "                urls = [link.get('href') for link in date_links]\n",
    "                selected_urls = [urls[i] for i in dates]\n",
    "\n",
    "                #Create the base url thta is added on to\n",
    "                pre_url = \"https://www.sports-reference.com/\"\n",
    "\n",
    "                #Add each url ending to the pre url to have a list of the full URLs\n",
    "                selected_urls = [pre_url + su for su in selected_urls]\n",
    "\n",
    "                #Create an empty dataframe to score last five games data\n",
    "                last_five = pd.DataFrame()\n",
    "\n",
    "                #Iterate through all five URLs\n",
    "                for link in selected_urls:\n",
    "\n",
    "                    #Use the scrape game data function to scrape the team totals for each game\n",
    "                    one_game = scrape_game_data(link, team)\n",
    "\n",
    "                    #Sleep for a random time so we are not locked out\n",
    "                    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "                    #If the game table isn't found then print that it was not\n",
    "                    if isinstance(one_game, str):\n",
    "                        print(\"Game Table not Found\")\n",
    "                    else:\n",
    "                        #Add each game to the last_five dataframe\n",
    "                        last_five = pd.concat([last_five, one_game], ignore_index = True)\n",
    "\n",
    "                #Only include this team if all five of their last games were scrapeable\n",
    "                if len(last_five) != 5:\n",
    "                    print(\"Not Scrapeable\")\n",
    "                    continue\n",
    "                \n",
    "                ##Change all the values to numeric\n",
    "                last_five = last_five.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "                #Drop the following two columns when they exist\n",
    "                #Take the mean of each column across the five games and turn this into a new dataframe\n",
    "                columns_to_drop = [\"Player\", \"GmSc\"]\n",
    "                five_game_average = pd.DataFrame([last_five.drop(columns = [col for col in columns_to_drop if col in last_five.columns]).mean()])\n",
    "\n",
    "                five_game_average[\"team\"] = team\n",
    "                \n",
    "                #Add the five_game average values onto the end of the tournament games\n",
    "                tournament_games = pd.merge(tournament_games, five_game_average, on = \"team\", how = \"left\", suffixes = ('', '_last_five_reg'))\n",
    "\n",
    "                #Rename the columns so they have the same suffix for each from the last_five table\n",
    "                for col in five_game_average.columns:\n",
    "                    if col != \"team\":\n",
    "                        tournament_games.rename(columns = {col: col + \"_last_five_reg\"}, inplace = True)\n",
    "\n",
    "                #Create strength of schedule in their last five games and how many wins they got\n",
    "                tournament_games[\"SRS_last_five_reg\"] = pd.to_numeric(regular_season_end[\"SRS\"], errors=\"coerce\").mean()\n",
    "                tournament_games[\"last_five_wins\"] = regular_season_end.iloc[:, 8].value_counts().get(\"W\", 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #Do the same thing with the conference tournament games if they exist\n",
    "                if (len(all_games[all_games[\"Type\"] == \"CTOURN\"]) > 0):\n",
    "\n",
    "                    conf_tournament_games = all_games[all_games[\"Type\"] == \"CTOURN\"]\n",
    "\n",
    "                    conf_tournament_games[\"G\"] = pd.to_numeric(conf_tournament_games[\"G\"])\n",
    "                    conf_tournament_dates = (conf_tournament_games[\"G\"] - 1).tolist()\n",
    "\n",
    "                    # Find all the clickable links in the Date column\n",
    "                    conf_tournament_date_links = soup.select(\"td[data-stat='date_game'] a\")\n",
    "\n",
    "                    conf_tournament_urls = [conf_tournament_link.get('href') for conf_tournament_link in conf_tournament_date_links]\n",
    "\n",
    "                    # Print the extracted URLs\n",
    "                    conf_tournament_selected_urls = [conf_tournament_urls[i] for i in conf_tournament_dates]\n",
    "\n",
    "                    conf_tournament_selected_urls = [pre_url + su for su in conf_tournament_selected_urls]\n",
    "\n",
    "                    conf_tournament = pd.DataFrame()\n",
    "                    for link in conf_tournament_selected_urls:\n",
    "                        one_conf_tournament_game = scrape_game_data(link, team)\n",
    "                        time.sleep(random.uniform(2, 5))\n",
    "                        if isinstance(one_conf_tournament_game, str):\n",
    "                            print(\"Conference Tournament Game Table not Found\")\n",
    "                        else:\n",
    "                            conf_tournament = pd.concat([conf_tournament, one_conf_tournament_game], ignore_index = True)\n",
    "\n",
    "                    if len(conf_tournament) < 1:\n",
    "                        print(\"Not Scrapeable\")\n",
    "                        continue\n",
    "\n",
    "                    conf_tournament = conf_tournament.apply(pd.to_numeric, errors='coerce')\n",
    "                \n",
    "                    conf_tournament_average = pd.DataFrame([conf_tournament.drop(columns=[col for col in columns_to_drop if col in conf_tournament.columns]).mean()])\n",
    "\n",
    "                    conf_tournament_average[\"team\"] = team\n",
    "\n",
    "                    tournament_games = pd.merge(tournament_games, conf_tournament_average, on = \"team\", how = \"left\", suffixes = ('', '_conf_tournament'))\n",
    "\n",
    "                    for col in conf_tournament_average.columns:\n",
    "                        if col != \"team\":\n",
    "                            tournament_games.rename(columns={col: col + \"_conf_tournament\"}, inplace=True)\n",
    "\n",
    "                    tournament_games[\"SRS_conf_tournament\"] = pd.to_numeric(conf_tournament_games[\"SRS\"], errors=\"coerce\").mean()\n",
    "                    tournament_games[\"wins_conf_tournament\"] = conf_tournament_games.iloc[:, 8].value_counts().get(\"W\", 0)\n",
    "                    tournament_games[\"conf_tournament_champ\"] = 1 if conf_tournament_games.iloc[:, 8].value_counts().get(\"L\", 0) == 0 else 0\n",
    "\n",
    "                    #Combine the conference tournament games and the last five regular season games into one dataframe\n",
    "                    season_end = pd.concat([conf_tournament, last_five], ignore_index = True)\n",
    "\n",
    "                    #Calculate the mean of all of these games \n",
    "                    season_end_average = pd.DataFrame([season_end.drop(columns=[col for col in columns_to_drop if col in season_end.columns]).mean()])\n",
    "\n",
    "                    season_end_average[\"team\"] = team\n",
    "\n",
    "                    #Merge them into the tournament games\n",
    "                    tournament_games = pd.merge(tournament_games, season_end_average, on = \"team\", how = \"left\", suffixes = ('', '_season_end'))\n",
    "\n",
    "                    #Add a suffix\n",
    "                    for col in season_end_average.columns:\n",
    "                        if col != \"team\":\n",
    "                            tournament_games.rename(columns={col: col + \"_season_end\"}, inplace=True)\n",
    "\n",
    "                    #Calculate the amount of wins and losses the team had in their last five regualar season and conference tournament games\n",
    "                    tournament_games[\"wins_season_end\"] = tournament_games[\"wins_conf_tournament\"] + tournament_games[\"last_five_wins\"]\n",
    "                    tournament_games[\"losses_season_end\"] = np.where(tournament_games[\"conf_tournament_champ\"] == 1, \n",
    "                                                                (5 - tournament_games[\"last_five_wins\"]), \n",
    "                                                                (6 - tournament_games[\"last_five_wins\"]))\n",
    "\n",
    "                    #Print the amount of rows of the full data frame before appending\n",
    "                    print(f\"Rows before appending: {len(all_tournament_games)}\")\n",
    "                    if not tournament_games.empty:\n",
    "\n",
    "                        #Append the tournament games to all the tournament games from past iterations\n",
    "                        all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
    "\n",
    "                        #Pring the amount of rows after the iteration as well as the number of teams that hvae been added\n",
    "                        teams_added = teams_added + 1\n",
    "                        print(f\"Rows after appending: {len(all_tournament_games)}\")\n",
    "                        print(teams_added, \"Teams have been added\")\n",
    "\n",
    "                #If for some reason, the team did not play in a conference tournament do the following\n",
    "                else:\n",
    "                    \n",
    "                    #Create a dataframe of zeros with the team column being the only one filled\n",
    "                    conf_tournament_average = pd.DataFrame([0], columns = conf_tournament_average.columns)\n",
    "                    conf_tournament_average[\"team\"] = team\n",
    "\n",
    "                    #Merge this zero dataframe to the tournament games\n",
    "                    tournament_games = pd.merge(tournament_games, conf_tournament_average, on = \"team\", how = \"left\", suffixes = ('', '_conf_tournament'))\n",
    "\n",
    "                    #Add the suffix\n",
    "                    for col in conf_tournament_average.columns:\n",
    "                        if col != \"team\":\n",
    "                            tournament_games.rename(columns={col: col + \"_conf_tournament\"}, inplace=True)\n",
    "                    \n",
    "                    #Make all the other columns zero as well\n",
    "                    tournament_games[\"SRS_conf_tournament\"] = 0\n",
    "                    tournament_games[\"wins_conf_tournament\"] = 0\n",
    "                    tournament_games[\"conf_tournament_champ\"] = 0\n",
    "\n",
    "                    #The following is the same as above, but now the season_end dataframe will only be the last five regualr season games since there were no conference tournament games\n",
    "                    season_end = last_five.copy()\n",
    "\n",
    "                    season_end_average = pd.DataFrame([season_end.drop(columns=[col for col in columns_to_drop if col in season_end.columns]).mean()])\n",
    "\n",
    "                    season_end_average[\"team\"] = team\n",
    "\n",
    "                    tournament_games = pd.merge(tournament_games, season_end_average, on = \"team\", how = \"left\", suffixes = ('', '_season_end'))\n",
    "\n",
    "                    for col in season_end_average.columns:\n",
    "                        if col != \"team\":\n",
    "                            tournament_games.rename(columns={col: col + \"_season_end\"}, inplace=True)\n",
    "\n",
    "                    tournament_games[\"wins_season_end\"] = tournament_games[\"wins_conf_tournament\"] + tournament_games[\"last_five_wins\"]\n",
    "                    tournament_games[\"losses_season_end\"] = np.where(tournament_games[\"conf_tournament_champ\"] == 1, \n",
    "                                                                (5 - tournament_games[\"last_five_wins\"]), \n",
    "                                                                (6 - tournament_games[\"last_five_wins\"]))\n",
    "\n",
    "                    print(f\"Rows before appending: {len(all_tournament_games)}\")\n",
    "                    if not tournament_games.empty:\n",
    "                        all_tournament_games = pd.concat([tournament_games, all_tournament_games], ignore_index=True)\n",
    "                        print(f\"Rows after appending: {len(all_tournament_games)}\")\n",
    "\n",
    "\n",
    "\n",
    "            #Sleep for 2 to 5 seconds before scraping again to not get locked out and print how long it slept for\n",
    "            sleep_time = random.uniform(2, 5)\n",
    "            print(f\"Iteration {idx}: Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        #Output any error that happened while scraping a given iteration\n",
    "        except Exception as e:\n",
    "            print(f\"Error before printing: {e}\")\n",
    "            traceback.print_exc()\n",
    "            print(\"Skipping to the next team...\\n\")\n",
    "            continue\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chunk of code does the actual scraping of our trend variables. The code first scrapes a given team and season url, using the team ids scraped from before. With this link, we create a table of all the team's games, subset the NCAA tournament games (the ones we will be predicting), conference tournament games, and last five regular season games. For the conference tournament and last five games, we used the function created previously on each game in both dataframes. This gives the team totals for each game entered in. When we have a dataframe of team totals from all the games, we calculate the average across them all and merge this to the NCAA tournament games. We do the same thing with the conference tournament and last five regular season games combined. Now, we have a row for each NCAA tournament game since 2002, with data on the team's performance in the last five regular season games, conference tournament games, and both combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Team Season-Long Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_season_stats = pd.DataFrame()\n",
    "\n",
    "#Now scrape through the seasons only since the given URL has stats for each team\n",
    "for season in seasons:\n",
    "  \n",
    "  #Store the url with the given season\n",
    "  url = f\"https://www.sports-reference.com/cbb/seasons/men/{season}-school-stats.html\"\n",
    "\n",
    "  #Headers to mimic a real browser request\n",
    "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "  #Retrieve the page from the link\n",
    "  response = requests.get(url, headers=headers)\n",
    "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "  #Find the basic stats table on this link\n",
    "  table = soup.find(\"table\", {\"id\": \"basic_school_stats\"})\n",
    "\n",
    "  #Extract the rows from the basic stats table\n",
    "  rows = []\n",
    "  team_codes = []\n",
    "  for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "      row = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "\n",
    "      #Extract the team id for merging from the link that is in the team column\n",
    "      school_td = tr.find(\"td\", {\"data-stat\": \"school_name\"})\n",
    "      if school_td:\n",
    "          team_link = school_td.find(\"a\")\n",
    "          if team_link:\n",
    "              \n",
    "              #Extract the actual link\n",
    "              team_url = team_link[\"href\"]\n",
    "\n",
    "              #Find the team code in the url\n",
    "              team_code = team_url.split(\"/\")[-3]\n",
    "          else:\n",
    "              \n",
    "              #If either wasn't found, then make the team_code column blank\n",
    "              team_code = None\n",
    "      else:\n",
    "          team_code = None\n",
    "      \n",
    "      #Append the rows and team codes\n",
    "      if row:\n",
    "          rows.append(row)\n",
    "          team_codes.append(team_code)\n",
    "\n",
    "  #Extract the number of columns and headers\n",
    "  num_cols = len(rows[0])\n",
    "  headers = [th.text.strip() for th in table.find(\"thead\").find_all(\"th\")]\n",
    "\n",
    "  #Ensure header count matches the data\n",
    "  if len(headers) != num_cols:\n",
    "      headers = [\"Column_\" + str(i) for i in range(num_cols)]\n",
    "\n",
    "  #Create a dataframe from the rows and column names\n",
    "  basic = pd.DataFrame(rows, columns = headers)\n",
    "  \n",
    "  #Store the actual column names\n",
    "  col_names = [\"School\", \"G\", \"W\", \"L\", \"W/L%\", \"SRS\", \"SOS\", \"blank1\", \"conf_w\", \"conf_l\", \"blank2\", \"home_w\", \"home_l\", \"blank3\", \"away_w\", \"away_l\", \"blank4\", \"team_points\", \"opponent_points\", \"blank\", \"mp\", \"fg\", \"fga\",\n",
    "              \"fgper\", \"3p\", \"3pa\", \"3p_per\", \"ft\", \"fta\", \"ftper\", \"orb\", \"trb\", \"ast\", \"stl\", \"blk\", \"tov\", \"pf\"]\n",
    "  \n",
    "  #Assign the column names to the actual column names\n",
    "  basic.columns = col_names\n",
    "\n",
    "  #Extract the schools that have NCAA in their school column, indicating they were in the NCAA tournament\n",
    "  basic = basic[basic['School'].str.contains('NCAA', case=False, na=False)]\n",
    "  \n",
    "  #Remove this NCAA from each of these school names\n",
    "  basic['School'] = basic['School'].str.replace('NCAA', '', regex=False)\n",
    "\n",
    "  basic[\"Season\"] = season\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  #Do the same thing, but now for the advanced school stats\n",
    "  url = f\"https://www.sports-reference.com/cbb/seasons/men/{season}-advanced-school-stats.html\"\n",
    "\n",
    " \n",
    "  headers = {\n",
    "      \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "  }\n",
    "\n",
    " \n",
    "  response = requests.get(url, headers=headers)\n",
    "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    " \n",
    "  table = soup.find(\"table\", {\"id\": \"adv_school_stats\"})\n",
    "\n",
    "\n",
    "  rows = []\n",
    "  team_codes = []\n",
    "  for tr in table.find(\"tbody\").find_all(\"tr\"):\n",
    "      row = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "\n",
    "\n",
    "      school_td = tr.find(\"td\", {\"data-stat\": \"school_name\"})\n",
    "      if school_td:\n",
    "          team_link = school_td.find(\"a\")\n",
    "          if team_link:\n",
    "              team_url = team_link[\"href\"]\n",
    "              team_code = team_url.split(\"/\")[-3] \n",
    "          else:\n",
    "              team_code = None \n",
    "      else:\n",
    "          team_code = None \n",
    "\n",
    "      if row:\n",
    "          rows.append(row)\n",
    "          team_codes.append(team_code)\n",
    "\n",
    "\n",
    "  num_cols = len(rows[0])\n",
    "  headers = [th.text.strip() for th in table.find(\"thead\").find_all(\"th\")]\n",
    "\n",
    "  if len(headers) != num_cols:\n",
    "      headers = [\"Column_\" + str(i) for i in range(num_cols)]\n",
    "\n",
    "  advanced = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "  col_names = [\"School\", \"G\", \"W\", \"L\", \"W/L%\", \"SRS\", \"SOS\", \"blk1\", \"conf_w\", \"conf_l\", \"blk2\", \"home_w\", \"home_l\", \"blk3\", \"away_w\", \"away_l\", \"blk4\", \"pts_scored\", \"pts_allowed\", \"blk5\", \"pace\", \"ortg\", \"FTr\",\n",
    "              \"3PAr\", \"true_shooting\", \"rebound_percentage\", \"assist_percentage\", \"steal_percentage\", \"block_percentage\", \"eFG\", \"tov_percentage\", \"oreb_percentage\", \"ft_per_fga\"]\n",
    "\n",
    "  advanced.columns = col_names\n",
    "\n",
    "  advanced[\"team_code\"] = team_codes\n",
    "\n",
    "  advanced = advanced[advanced['School'].str.contains('NCAA', case=False, na=False)]\n",
    "\n",
    "  advanced['School'] = advanced['School'].str.replace('NCAA', '', regex=False)\n",
    "\n",
    "  \n",
    "  #Merge the advanced data and basic data on school\n",
    "  one_season = pd.merge(basic, advanced, on=['School'], how = \"left\")\n",
    "\n",
    "  #Combine the season stats to all previous iterations\n",
    "  all_season_stats = pd.concat([all_season_stats, one_season])\n",
    "\n",
    "  #Output the season that has been scraped\n",
    "  print(f\"Scraped {season} Season\")\n",
    "  \n",
    "  #Sleep for a random time between 2 and 5 seconds and print the length\n",
    "  sleep_time = random.uniform(2, 5)\n",
    "  print(f\"Sleeping for {sleep_time:.2f} seconds...\")\n",
    "  time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code scrapes season long data for each team in the given seasons. It scrapes basic stats along with advanced stats and puts them together. The data also scrapes each team's team id so that it can be easily merged with out other dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the all tournament games with the trend data with the season long stats data\n",
    "#Merge on team and season with a left join so that we only include teams that we have games for\n",
    "merged_data = pd.merge(all_tournament_games, all_season_stats, left_on = [\"team\", \"season\"], right_on = [\"team_code\", \"Season\"], how = \"left\")\n",
    "\n",
    "#Create a column that combines the date and time \n",
    "merged_data[\"date_time\"] = merged_data[\"Date\"] + \" \" + merged_data[\"Time\"]\n",
    "\n",
    "#Use this column to merge the data with itself, adding a suffix\n",
    "expanded_merged = merged_data.merge(merged_data, on = \"date_time\", suffixes = (\"\", \"_opponent\"), how = \"left\")\n",
    "\n",
    "#Remove the rows where the team is \"playing itself\"\n",
    "expanded_merged = expanded_merged[expanded_merged[\"team\"] != expanded_merged[\"team_opponent\"]]\n",
    "\n",
    "#Output the data to a csv\n",
    "expanded_merged.to_csv(\"merged_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code merges our two data frames together. At that point, we have two rows for each game. In order to make only one row for each game, we merge the data to itself, combining all games to themselves. In this step, we add an opponent suffix so that you know those columns are for the opponent. From here, we remove all the rows where the team joined with itself. \n",
    "\n",
    "At this point, we have our final dataset. This dataset has a row for each NCAA tournament game since 2002 that we were able to scrape. Each row has a random team as the \"point of view\" team, along with their stats, their opponent's stats, and whether or not they won the game (our response variable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
